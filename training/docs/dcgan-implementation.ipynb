{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":786434,"sourceType":"datasetVersion","datasetId":410745}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Trying to Implement dcGAN using PyTorch**: _Pokemon Dataset_\n---","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"## Imports \nimport shutil\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom PIL import Image, ImageFile\nimport glob\nimport os \nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:04:30.417982Z","iopub.execute_input":"2024-12-25T13:04:30.418307Z","iopub.status.idle":"2024-12-25T13:04:34.208393Z","shell.execute_reply.started":"2024-12-25T13:04:30.418272Z","shell.execute_reply":"2024-12-25T13:04:34.207716Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:04:34.209535Z","iopub.execute_input":"2024-12-25T13:04:34.209958Z","iopub.status.idle":"2024-12-25T13:04:34.278013Z","shell.execute_reply.started":"2024-12-25T13:04:34.209926Z","shell.execute_reply":"2024-12-25T13:04:34.277077Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"So before we dive into the DCGAN pipeline, lets first create a suitable dataset path and since we're not going to generate in classified way, we will first load all the images and normalize them before processing ","metadata":{}},{"cell_type":"code","source":"pokemon_data_path = \"/kaggle/input/pokemonclassification/PokemonData\"\n\n#@ Collecting only jpg, jpng and png files from the dataset\nimage_files = glob.glob(os.path.join(pokemon_data_path, \"*\", \"*.jpg\")) + \\\n              glob.glob(os.path.join(pokemon_data_path, \"*\", \"*.jpeg\")) + \\\n              glob.glob(os.path.join(pokemon_data_path, \"*\", \"*.png\"))\n\nprint(f\"Total images found: {len(image_files)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:04:34.279906Z","iopub.execute_input":"2024-12-25T13:04:34.280163Z","iopub.status.idle":"2024-12-25T13:04:35.806607Z","shell.execute_reply.started":"2024-12-25T13:04:34.280141Z","shell.execute_reply":"2024-12-25T13:04:35.805827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@ Preprocessing the data:\n\n### Normalizing the RGB data after resizing them\ntransform = transforms.Compose([\n    transforms.Resize((64,64)), \n    transforms.ToTensor(), \n    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5]) \n])\n\n### The custom class for Pokemon images:\nclass PokemonDataset(Dataset):\n    def __init__(self, image_files, transform= None):\n        self.image_files = image_files\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_files[idx]).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image\n\n## Creating the dataset and DataLoader:\npokemon_dataset = PokemonDataset(image_files, transform= transform)\ndataloader = DataLoader(pokemon_dataset, batch_size= 128, shuffle= True, num_workers=4, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:04:35.80787Z","iopub.execute_input":"2024-12-25T13:04:35.808105Z","iopub.status.idle":"2024-12-25T13:04:35.814147Z","shell.execute_reply.started":"2024-12-25T13:04:35.808085Z","shell.execute_reply":"2024-12-25T13:04:35.8132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##@ Checking the sample\ndata_iter = iter(dataloader)\nimages= next(data_iter)\nprint(f\"Batch shape: {images.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:04:35.815089Z","iopub.execute_input":"2024-12-25T13:04:35.815382Z","iopub.status.idle":"2024-12-25T13:04:37.381997Z","shell.execute_reply.started":"2024-12-25T13:04:35.815353Z","shell.execute_reply":"2024-12-25T13:04:37.379303Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Batch_size= 128\n- Number_of_channels= 3\n- height = 64\n- width= 64","metadata":{}},{"cell_type":"code","source":"##@ Lets as well as visualize the images loaded\n\ngrid = vutils.make_grid(images[:64], nrow= 8, normalize= True)\nplt.figure(figsize=(7,7))\nplt.imshow(grid.permute(1,2,0))\nplt.axis(\"off\")\nplt.title(\"Sample Pok√©mon Images\", fontsize=16, color='#3c68b0', fontweight='bold', loc='center')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:04:37.384015Z","iopub.execute_input":"2024-12-25T13:04:37.385352Z","iopub.status.idle":"2024-12-25T13:04:37.927869Z","shell.execute_reply.started":"2024-12-25T13:04:37.38532Z","shell.execute_reply":"2024-12-25T13:04:37.926814Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Creating the Architecture\n---","metadata":{}},{"cell_type":"markdown","source":"Defining all the required hyperparameters first.","metadata":{}},{"cell_type":"code","source":"##@ Hyperparameters:\n\nnz= 100\nngf = 64 ##! Generator feature maps\nndf = 64 ##! Discriminator feature maps\nnc= 3\n\nkernel_size = 4\n\n## Training parameters\nbatch_size= 128\nlr= 0.0002  #Authors found that 0.001 was too high \nbeta1= 0.5 \nbeta2 = 0.999\nnum_epochs = 100\nsample_interval = 500","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:04:37.92875Z","iopub.execute_input":"2024-12-25T13:04:37.92905Z","iopub.status.idle":"2024-12-25T13:04:37.934281Z","shell.execute_reply.started":"2024-12-25T13:04:37.929023Z","shell.execute_reply":"2024-12-25T13:04:37.933475Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 1. Creating the Generator","metadata":{}},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, nz, ngf, nc):\n        super(Generator, self).__init__()\n\n        self.main = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf*8, kernel_size, 1, 0, bias= False),\n            nn.BatchNorm2d(ngf*8), \n            nn.ReLU(inplace= True), \n            nn.ConvTranspose2d(ngf*8, ngf*4, kernel_size, 2, 1, bias= False), \n            nn.BatchNorm2d(ngf*4), \n            nn.ReLU(inplace= True), \n            nn.ConvTranspose2d(ngf*4, ngf* 2, kernel_size, 2, 1, bias= False), \n            nn.BatchNorm2d(ngf*2), \n            nn.ReLU(inplace= True), \n            nn.ConvTranspose2d(ngf*2, ngf, kernel_size, 2, 1, bias= False), \n            nn.BatchNorm2d(ngf), \n            nn.ReLU(inplace= True), \n            nn.ConvTranspose2d(ngf, nc, kernel_size, 2, 1, bias= False), \n            nn.Tanh()\n            \n        )\n\n    def forward(self, input):\n        return self.main(input)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:04:37.93714Z","iopub.execute_input":"2024-12-25T13:04:37.937763Z","iopub.status.idle":"2024-12-25T13:04:37.951141Z","shell.execute_reply.started":"2024-12-25T13:04:37.937731Z","shell.execute_reply":"2024-12-25T13:04:37.948925Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 2. Creating the Discriminator","metadata":{}},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, nc, ndf):\n        super(Discriminator, self).__init__()\n\n        self.main= nn.Sequential(\n            nn.Conv2d(nc, ndf, kernel_size, 2, 1, bias= False), \n            nn.LeakyReLU(0.2, inplace= True), \n            nn.Conv2d(ndf, ndf*2, kernel_size, 2, 1, bias= False), \n            nn.BatchNorm2d(ndf*2), \n            nn.LeakyReLU(0.2, inplace= True), \n            nn.Conv2d(ndf*2, ndf*4, kernel_size, 2, 1, bias= False), \n            nn.BatchNorm2d(ndf*4), \n            nn.LeakyReLU(0.2, inplace= True), \n            nn.Conv2d(ndf*4, ndf*8, kernel_size, 2, 1, bias= False), \n            nn.BatchNorm2d(ndf*8), \n            nn.LeakyReLU(0.2, inplace= True), \n            nn.Conv2d(ndf*8, 1, kernel_size, 1, 0, bias= False), \n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:04:37.953363Z","iopub.execute_input":"2024-12-25T13:04:37.953671Z","iopub.status.idle":"2024-12-25T13:04:37.970423Z","shell.execute_reply.started":"2024-12-25T13:04:37.953644Z","shell.execute_reply":"2024-12-25T13:04:37.969255Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 3. Loss Function","metadata":{}},{"cell_type":"code","source":"adversarial_loss = torch.nn.BCELoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:04:37.971436Z","iopub.execute_input":"2024-12-25T13:04:37.971709Z","iopub.status.idle":"2024-12-25T13:04:37.994646Z","shell.execute_reply.started":"2024-12-25T13:04:37.971679Z","shell.execute_reply":"2024-12-25T13:04:37.993882Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 4. Initialization","metadata":{}},{"cell_type":"code","source":"##@  Initializing generator and discriminator\ngenerator = Generator(nz, ngf, nc).to(device)\ndiscriminator = Discriminator(nc, ndf).to(device)\n\nprint(generator)\nprint(discriminator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:04:37.995269Z","iopub.execute_input":"2024-12-25T13:04:37.99555Z","iopub.status.idle":"2024-12-25T13:04:38.118997Z","shell.execute_reply.started":"2024-12-25T13:04:37.995523Z","shell.execute_reply":"2024-12-25T13:04:38.118204Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 5. Optimization","metadata":{}},{"cell_type":"code","source":"fixed_noise= torch.randn(64, nz, 1,1, device= device)\n\noptimizer_g= torch.optim.Adam(generator.parameters(), lr= lr, betas=(beta1, beta2))\noptimizer_d= torch.optim.Adam(discriminator.parameters(), lr= lr, betas=(beta1, beta2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:04:38.11961Z","iopub.execute_input":"2024-12-25T13:04:38.119903Z","iopub.status.idle":"2024-12-25T13:04:38.141271Z","shell.execute_reply.started":"2024-12-25T13:04:38.119873Z","shell.execute_reply":"2024-12-25T13:04:38.140276Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 6. Training Loop","metadata":{}},{"cell_type":"code","source":"#@ Directory to save the generated images\nif os.path.exists('dc_images'):\n  shutil.rmtree('dc_images')   ## will delete the directory and its contents\nos.makedirs(\"dc_images\", exist_ok= True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:04:38.142035Z","iopub.execute_input":"2024-12-25T13:04:38.142322Z","iopub.status.idle":"2024-12-25T13:04:38.152248Z","shell.execute_reply.started":"2024-12-25T13:04:38.142292Z","shell.execute_reply":"2024-12-25T13:04:38.14687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Starting training ....\")\n\nfor epoch in range(num_epochs):\n    for i, imgs in enumerate(dataloader):\n\n        # Adversarial ground truths (real and fake labels)\n        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0).to(device), requires_grad=False)\n        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0).to(device), requires_grad=False)\n\n        # Configure input\n        real_imgs = imgs.to(device)  # Move real images directly to device\n\n        # --- Training the Generator ---\n        optimizer_g.zero_grad()\n\n        # Sample noise as generator input\n        z = Variable(Tensor(np.random.normal(0, 1, (imgs.size(0), nz, 1, 1)))).to(device)\n\n        # Generate a batch of images\n        gen_imgs = generator(z)\n\n        # Loss measures generator's ability to fool the discriminator\n        g_loss = adversarial_loss(discriminator(gen_imgs).view(-1, 1), valid)\n\n        g_loss.backward()\n        optimizer_g.step()\n\n        # --- Training the Discriminator ---\n        optimizer_d.zero_grad()\n\n        # Measure discriminator's ability to classify real from generated samples\n        real_loss = adversarial_loss(discriminator(real_imgs).view(-1, 1), valid)\n        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()).view(-1, 1), fake)\n        d_loss = (real_loss + fake_loss) / 2\n\n        d_loss.backward()\n        optimizer_d.step()\n\n        # Print training progress\n        # print(\n        #     \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n        #     % (epoch + 1, num_epochs, i + 1, len(dataloader), d_loss.item(), g_loss.item())\n        # )\n        ## Commenting out the print so that it takes lesser time to save in kaggle as well as it makes notebook clean\n\n        # Save generated samples only for batch 0 of each epoch\n        if i == 0:  # Only save for the first batch of each epoch\n            save_image(gen_imgs.data[:25], f\"dc_images/epoch_{epoch}_batch_{i}.png\", nrow=5, normalize=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@ Viewing the generated outputs progressments\ninitial_path = f\"dc_images/epoch_0_batch_0.png\"\nmiddle_path = f\"dc_images/epoch_{num_epochs // 2}_batch_0.png\"\nfinal_path = f\"dc_images/epoch_{num_epochs-1}_batch_0.png\"\n\n# Function to load and display an image from a file path\ndef show_image(image_path, title=\"Image\"):\n    if os.path.exists(image_path):\n        img = mpimg.imread(image_path)\n        \n        # Dynamically adjust the figure size based on the image's shape\n        height, width, _ = img.shape\n        plt.figure(figsize=(width / 100, height / 100))  # Adjust size based on image dimensions\n        \n        plt.imshow(img)\n        plt.title(title)\n        plt.axis(\"off\")  # Remove axes for better visualization\n        plt.show()\n    else:\n        print(f\"Image not found: {image_path}\")\n\n# Display initial, middle, and final images\nshow_image(initial_path, title=\"Initial Epoch (0)\")\nshow_image(middle_path, title=f\"Middle Epoch ({num_epochs // 2})\")\nshow_image(final_path, title=f\"Final Epoch ({num_epochs - 1})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:31:23.84105Z","iopub.execute_input":"2024-12-25T13:31:23.841359Z","iopub.status.idle":"2024-12-25T13:31:24.378298Z","shell.execute_reply.started":"2024-12-25T13:31:23.84133Z","shell.execute_reply":"2024-12-25T13:31:24.377462Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Well, it seems like it's working.. ü§∑‚Äç‚ôÇÔ∏è","metadata":{}}]}